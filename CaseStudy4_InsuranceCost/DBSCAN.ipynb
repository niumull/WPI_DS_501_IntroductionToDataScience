{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn import cluster\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial import distance\n",
    "import collections\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed.cleveland_ini_pre.csv')\n",
    "df_1 = pd.DataFrame(df, columns=['#3(age)','#4(sex)','#58(num)'])\n",
    "df_2 = pd.DataFrame(df, columns=['#9(cp)=TypicalAngina','#9(cp)=Asymptomatic','#9(cp)=NonAnginalPain','#9(cp)=AtypicalAngina',\n",
    "                                 '#16(fbs)','#32(thalach)'])\n",
    "df_target = pd.DataFrame(df, columns=['#58(num)'])\n",
    "df_target = np.concatenate(df_target.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ed1907221ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[1;31m## Merics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Normalized mutual information score is %.8s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnormalized_mutual_info_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mK_1_pre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'k_1' is not defined"
     ]
    }
   ],
   "source": [
    "DBSCAN = cluster.DBSCAN(metric='euclidean', algorithm='auto',eps=0.6,min_samples=10, p=None).fit(df)\n",
    "K_1_pre = [0]*len(df_target)\n",
    "for i in range(len(df_target)):\n",
    "    if df_target[i] == 0:\n",
    "        K_1_pre[i] = 0\n",
    "        \n",
    "    elif df_target[i] == 1:\n",
    "        K_1_pre[i] = 4\n",
    "        \n",
    "    elif df_target[i] == 2:\n",
    "        K_1_pre[i] = 2\n",
    "        \n",
    "    elif df_target[i] == 3:\n",
    "        K_1_pre[i] = 3\n",
    "        \n",
    "    elif df_target[i] == 4:\n",
    "        K_1_pre[i] = 1\n",
    "\n",
    "\n",
    "## Merics\n",
    "print(\"Normalized mutual information score is %.8s.\" % (normalized_mutual_info_score(k_1.labels_,K_1_pre))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage of clusters\n",
      "29.966329966329965\n",
      "9.427609427609427\n",
      "15.151515151515152\n",
      "23.905723905723907\n",
      "9.427609427609427\n",
      "3.7037037037037037\n",
      "0.0\n",
      "{'random_state': None, 'leaf_size': 30, 'min_samples': 10, 'metric': 'euclidean', 'algorithm': 'auto', 'eps': 0.6, 'p': None}\n",
      "Counter({0: 89, 3: 71, 2: 45, 1: 28, 4: 28, -1: 25, 5: 11})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004000425338745117"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "DBSCAN = cluster.DBSCAN(metric='euclidean', algorithm='auto',eps=0.6,min_samples=10, p=None).fit(df_1)\n",
    "A = len(collections.Counter(DBSCAN.labels_))\n",
    "cluster_percentage = [0]*A\n",
    "print()\n",
    "print('Percentage of clusters')\n",
    "for i in range(A):\n",
    "    print(collections.Counter(DBSCAN.labels_)[i]*100/297)\n",
    "    \n",
    "print(DBSCAN.get_params())\n",
    "print(collections.Counter(DBSCAN.labels_))\n",
    "load_time_d_1 = time.time() - start_time\n",
    "load_time_d_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  0,  3,  0, -1,  3,  1,  2,  0,  3,  1,  0,  0,  0,  2,\n",
       "        0,  3,  0,  0,  3,  2,  4,  5,  3,  3,  3,  0,  4,  3,  1,  2,  0,\n",
       "        0,  0,  4,  2,  4,  0, -1,  0,  3,  0, -1,  5,  0,  5,  3,  0,  3,\n",
       "        0,  1,  0,  2,  2,  2,  2,  0,  0, -1,  3,  2,  3,  1,  1,  2,  0,\n",
       "        1,  2,  3,  4,  2,  2,  2,  3,  2,  3,  0,  4,  0,  3,  0,  4,  0,\n",
       "        0,  0,  3,  3,  0, -1,  0,  3,  3,  2,  1, -1,  0,  0,  0,  0,  3,\n",
       "        3,  4,  0,  1,  2,  1,  4, -1,  2,  0, -1, -1,  0,  0,  3,  4,  1,\n",
       "        4, -1,  0,  4,  2,  3, -1,  4,  0,  3,  0,  0,  0,  0,  3,  3,  5,\n",
       "        4,  2,  0,  0,  2,  0,  2,  0,  2,  5,  0,  0,  3,  0,  3,  3,  5,\n",
       "        4,  2,  2,  2,  1,  0,  0,  5,  3,  3,  0,  0,  3,  2,  3,  4,  0,\n",
       "       -1,  3,  5,  2,  0,  2,  0,  0,  4, -1,  0,  0, -1,  3,  0,  1,  2,\n",
       "        1,  0,  4, -1,  3,  4,  0,  3,  3,  2,  3,  3,  0,  3,  0,  4,  4,\n",
       "        4,  0, -1,  3,  5,  0, -1,  2,  0,  3,  3,  3,  0,  3,  3,  3,  4,\n",
       "       -1,  3,  0,  3,  4,  1,  3, -1,  2,  3,  3,  4,  1,  2,  3,  0,  0,\n",
       "        3,  3,  1,  3,  1,  1,  2,  4,  0,  0,  2,  0,  3,  0,  3,  3,  3,\n",
       "        0,  2,  3, -1,  3,  0,  5,  1,  1,  2,  0,  2,  0,  1,  3,  2,  0,\n",
       "        3,  3,  2,  3,  1,  0, -1,  0,  1,  5, -1,  0,  0,  2,  3,  1,  1,\n",
       "       -1,  0,  4, -1,  2,  1,  4, -1], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DBSCAN.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1,  1,  0, -1, -1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  0,\n",
       "       -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  0, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1,  1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1,  0, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  0,  0,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1,\n",
       "       -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1,  1, -1, -1, -1, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1,  0, -1, -1, -1, -1, -1, -1], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DBSCAN.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  0  3  0  4  3  1  2  0  3  1  0  0  0  2  0  3  0  0  3  2  5  6\n",
      "  3  3  3  0  5  3  1  2  0  0  0  5  2  5  0 -1  0  3  0  7  6  0  6  3  0\n",
      "  3  0  1  0  2  2  2  2  0  0  8  3  2  3  1  1  2  0  1  2  3  5  2  2  2\n",
      "  3  2  3  0  5  0  3  0  5  0  0  0  3  3  0  4  0  3  3  2  1  4  0  0  0\n",
      "  0  3  3  5  0  1  2  1  5  7  2  0  8  8  0  0  3  5  1  5 -1  0  5  2  3\n",
      "  4  5  0  3  0  0  0  0  3  3  6  5  2  0  0  2  0  2  0  2  6  0  0  3  0\n",
      "  3  3  6  5  2  2  2  1  0  0  6  3  3  0  0  3  2  3  5  0  7  3  6  2  0\n",
      "  2  0  0  5  8  0  0  7  3  0  1  2  1  0  5  8  3  5  0  3  3  2  3  3  0\n",
      "  3  0  5  5  5  0  7  3  6  0  4  2  0  3  3  3  0  3  3  3  5  7  3  0  3\n",
      "  5  1  3  8  2  3  3  5  1  2  3  0  0  3  3  1  3  1  1  2  5  0  0  2  0\n",
      "  3  0  3  3  3  0  2  3  4  3  0  6  1  1  2  0  2  0  1  3  2  0  3  3  2\n",
      "  3  1  0  4  0  1  6  8  0  0  2  3  1  1  7  0  5  7  2  1  5  7]\n",
      "{'random_state': None, 'leaf_size': 30, 'min_samples': 5, 'metric': 'euclidean', 'algorithm': 'auto', 'eps': 0.5, 'p': None}\n",
      "0.6734006734006734\n",
      "29.966329966329965\n",
      "9.427609427609427\n",
      "15.151515151515152\n",
      "23.905723905723907\n",
      "2.356902356902357\n",
      "9.427609427609427\n",
      "3.7037037037037037\n",
      "3.0303030303030303\n",
      "2.356902356902357\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Normalized mutual information score is 1.0 for DBSCAN cluster.\n",
      "V-measure score is 1.0 for DBSCAN cluster.\n",
      "Average Silhouette Coefficient for cluster 0 is 0.6311.\n",
      "Average Silhouette Coefficient for cluster 1 is 0.0404.\n",
      "Average Silhouette Coefficient for cluster 2 is 0.0782.\n",
      "Average Silhouette Coefficient for cluster 3 is 0.0446.\n",
      "Average Silhouette Coefficient for cluster 4 is 0.1688.\n",
      "Average Silhouette Coefficient for cluster 5 is -0.095.\n",
      "Average Silhouette Coefficient for cluster 6 is 0.1198.\n",
      "Average Silhouette Coefficient for cluster 7 is -0.053.\n",
      "Average Silhouette Coefficient for cluster 8 is 0.2031.\n",
      "Average Silhouette Coefficient for cluster 9 is -0.010.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "DBSCAN = cluster.DBSCAN(metric='euclidean', algorithm='auto',eps=0.5,min_samples=5, p=None).fit(df_1)\n",
    "print(DBSCAN.labels_) \n",
    "pca = PCA(n_components=1)\n",
    "df_1_pca = np.ravel(pca.fit_transform(df_1))\n",
    "DDD = [df_1_pca.T,DBSCAN.labels_]\n",
    "df_1_db = pd.DataFrame(DDD)\n",
    "df_1_db = np.array(df_1_db.T.sort_values(by=[1], ascending=[True]).drop([1],1))\n",
    "df_1_distance = pairwise_distances(df_1_db, metric='euclidean')\n",
    "d_1_max = np.amax(df_1_distance)\n",
    "d_1_min = np.amin(df_1_distance)\n",
    "df_1_similarity = np.zeros([297,297])\n",
    "for i in range(len(df_1_distance)):\n",
    "    for j in range(len(df_1_distance)):\n",
    "        df_1_similarity[i,j] = 1-(df_1_distance[i,j]-d_1_min)/(d_1_max-d_1_min)\n",
    "\n",
    "# Similarity matrix\n",
    "sns.set(context=\"paper\", font=\"monospace\")\n",
    "f, ax = plt.subplots(figsize=(8,7))\n",
    "cmap = sns.diverging_palette(220, 8, as_cmap=True)\n",
    "sns.heatmap(df_1_similarity, cmap='GnBu',vmax=1, square=True)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "plt.title('Similarity matrix for DBSCAN cluster (Q1)',fontsize=15)\n",
    "plt.savefig('d1.jpg')\n",
    "\n",
    "print(DBSCAN.get_params())\n",
    "cluster_percentage = [0]*15\n",
    "for i in range(15):\n",
    "    print(collections.Counter(DBSCAN.labels_)[i-1]*100/297)\n",
    "    \n",
    "DBSCAN_p = cluster.DBSCAN(metric='euclidean', algorithm='auto', leaf_size=4, p=None).fit_predict(df_1)\n",
    "# print(mutual_info_score(DBSCAN.labels_,DBSCAN_p))\n",
    "print(\"Normalized mutual information score is %.8s for DBSCAN cluster.\" % (normalized_mutual_info_score(DBSCAN.labels_,DBSCAN_p)))\n",
    "print(\"V-measure score is %.8s for DBSCAN cluster.\" % (v_measure_score(DBSCAN.labels_,DBSCAN_p)))\n",
    "SS_value = [silhouette_samples(df,DBSCAN.labels_),DBSCAN.labels_]\n",
    "SS_value = pd.DataFrame(SS_value)\n",
    "SS_value = np.array(SS_value.T.sort_values(by=[1], ascending=[True]).drop([1],1))\n",
    "cluster_num = [0]*len(collections.Counter(DBSCAN.labels_))\n",
    "for i in range(len(cluster_num)):\n",
    "    cluster_num[i] = collections.Counter(DBSCAN.labels_)[i-1]\n",
    "cluster_cum = np.cumsum(cluster_num)\n",
    "\n",
    "ran = len(cluster_cum)\n",
    "for i in range(ran):\n",
    "    if i == 0:\n",
    "        print(\"Average Silhouette Coefficient for cluster %.6s is %.6s.\" % (i,np.mean(SS_value[0:cluster_cum[i]])))\n",
    "    else:\n",
    "        print(\"Average Silhouette Coefficient for cluster %.6s is %.6s.\" % (i,np.mean(SS_value[cluster_cum[i-1]:cluster_cum[i]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 2 3 3 1 1 1 4 1 3 5 3 5 2 3 1 2 3 6 0 3 2 1 2 2 6 1 1 6 4 2 1 2 1 1\n",
      " 1 1 5 1 6 3 5 1 2 2 1 5 5 3 1 1 3 1 1 2 1 2 6 1 2 1 5 1 1 2 2 1 2 2 4 1 1\n",
      " 1 2 1 2 3 1 1 1 2 5 3 2 2 1 2 1 1 2 2 2 1 1 1 3 1 1 6 1 5 2 3 1 2 1 1 1 4\n",
      " 6 4 2 3 5 1 4 1 4 1 2 1 0 3 4 1 3 1 2 2 3 1 2 3 1 3 1 5 3 6 7 2 2 2 4 2 3\n",
      " 2 0 1 2 1 1 1 1 1 1 2 7 1 2 1 5 1 7 1 3 2 1 1 1 1 1 4 1 2 5 1 1 6 6 1 3 5\n",
      " 3 3 2 2 1 4 2 1 0 1 3 6 1 1 5 2 1 1 1 1 3 1 2 6 2 4 1 6 3 1 1 1 2 2 2 1 1\n",
      " 3 1 2 1 1 2 1 2 3 2 1 4 1 3 3 3 3 1 6 5 1 1 1 1 7 1 1 1 2 1 2 1 2 3 3 2 7\n",
      " 6 2 1 1 5 1 2 1 1 1 1 6 6 2 2 3 1 1 2 1 3 1 1 4 3 3 2 3 1 1 1 3 4 1 6 4 1\n",
      " 3]\n",
      "{'random_state': None, 'leaf_size': 4, 'min_samples': 5, 'metric': 'euclidean', 'algorithm': 'auto', 'eps': 0.5, 'p': None}\n",
      "0.0\n",
      "1.6835016835016836\n",
      "42.42424242424242\n",
      "22.22222222222222\n",
      "14.814814814814815\n",
      "5.3872053872053876\n",
      "5.723905723905724\n",
      "6.0606060606060606\n",
      "1.6835016835016836\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Normalized mutual information score is 1.0 for DBSCAN cluster.\n",
      "V-measure score is 1.0 for DBSCAN cluster.\n",
      "Average Silhouette Coefficient for cluster 0 is 0.1937.\n",
      "Average Silhouette Coefficient for cluster 1 is -0.053.\n",
      "Average Silhouette Coefficient for cluster 2 is 0.1295.\n",
      "Average Silhouette Coefficient for cluster 3 is 0.2454.\n",
      "Average Silhouette Coefficient for cluster 4 is 0.1552.\n",
      "Average Silhouette Coefficient for cluster 5 is 0.0297.\n",
      "Average Silhouette Coefficient for cluster 6 is -0.024.\n",
      "Average Silhouette Coefficient for cluster 7 is -0.128.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "DBSCAN = cluster.DBSCAN(metric='euclidean', algorithm='auto', leaf_size=4, p=None).fit(df_2)\n",
    "print(DBSCAN.labels_) \n",
    "\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "df_2_pca = np.ravel(pca.fit_transform(df_2))\n",
    "DDD = [df_2_pca.T,DBSCAN.labels_]\n",
    "df_2_db = pd.DataFrame(DDD)\n",
    "df_2_db = np.array(df_2_db.T.sort_values(by=[1], ascending=[True]).drop([1],1))\n",
    "df_2_distance = pairwise_distances(df_2_db, metric='euclidean')\n",
    "d_2_max = np.amax(df_2_distance)\n",
    "d_2_min = np.amin(df_2_distance)\n",
    "df_2_similarity = np.zeros([297,297])\n",
    "for i in range(len(df_2_distance)):\n",
    "    for j in range(len(df_2_distance)):\n",
    "        df_2_similarity[i,j] = 1-(df_2_distance[i,j]-d_2_min)/(d_2_max-d_2_min)\n",
    "\n",
    "# Similarity matrix\n",
    "sns.set(context=\"paper\", font=\"monospace\")\n",
    "f, ax = plt.subplots(figsize=(8,7))\n",
    "cmap = sns.diverging_palette(220, 8, as_cmap=True)\n",
    "sns.heatmap(df_2_similarity, cmap='GnBu',vmax=1, square=True)\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "plt.title('Similarity matrix for DBSCAN cluster (Q2)',fontsize=15)\n",
    "plt.savefig('d2.jpg')\n",
    "\n",
    "print(DBSCAN.get_params())\n",
    "cluster_percentage = [0]*15\n",
    "for i in range(15):\n",
    "    print(collections.Counter(DBSCAN.labels_)[i-1]*100/297)\n",
    "    \n",
    "    \n",
    "DBSCAN_p = cluster.DBSCAN(metric='euclidean', algorithm='auto', leaf_size=4, p=None).fit_predict(df_2)\n",
    "# print(mutual_info_score(DBSCAN.labels_,DBSCAN_p))\n",
    "print(\"Normalized mutual information score is %.8s for DBSCAN cluster.\" % (normalized_mutual_info_score(DBSCAN.labels_,DBSCAN_p)))\n",
    "print(\"V-measure score is %.8s for DBSCAN cluster.\" % (v_measure_score(DBSCAN.labels_,DBSCAN_p)))\n",
    "SS_value = [silhouette_samples(df,DBSCAN.labels_),DBSCAN.labels_]\n",
    "SS_value = pd.DataFrame(SS_value)\n",
    "SS_value = np.array(SS_value.T.sort_values(by=[1], ascending=[True]).drop([1],1))\n",
    "cluster_num = [0]*len(collections.Counter(DBSCAN.labels_))\n",
    "for i in range(len(cluster_num)):\n",
    "    cluster_num[i] = collections.Counter(DBSCAN.labels_)[i]\n",
    "cluster_cum = np.cumsum(cluster_num)\n",
    "\n",
    "ran = len(cluster_cum)\n",
    "for i in range(ran):\n",
    "    if i == 0:\n",
    "        print(\"Average Silhouette Coefficient for cluster %.6s is %.6s.\" % (i,np.mean(SS_value[0:cluster_cum[i]])))\n",
    "    else:\n",
    "        print(\"Average Silhouette Coefficient for cluster %.6s is %.6s.\" % (i,np.mean(SS_value[cluster_cum[i-1]:cluster_cum[i]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 2 3 3 1 1 1 4 1 3 5 3 5 2 3 1 2 3 6 0 3 2 1 2 2 6 1 1 6 4 2 1 2 1 1\n",
      " 1 1 5 1 6 3 5 1 2 2 1 5 5 3 1 1 3 1 1 2 1 2 6 1 2 1 5 1 1 2 2 1 2 2 4 1 1\n",
      " 1 2 1 2 3 1 1 1 2 5 3 2 2 1 2 1 1 2 2 2 1 1 1 3 1 1 6 1 5 2 3 1 2 1 1 1 4\n",
      " 6 4 2 3 5 1 4 1 4 1 2 1 0 3 4 1 3 1 2 2 3 1 2 3 1 3 1 5 3 6 7 2 2 2 4 2 3\n",
      " 2 0 1 2 1 1 1 1 1 1 2 7 1 2 1 5 1 7 1 3 2 1 1 1 1 1 4 1 2 5 1 1 6 6 1 3 5\n",
      " 3 3 2 2 1 4 2 1 0 1 3 6 1 1 5 2 1 1 1 1 3 1 2 6 2 4 1 6 3 1 1 1 2 2 2 1 1\n",
      " 3 1 2 1 1 2 1 2 3 2 1 4 1 3 3 3 3 1 6 5 1 1 1 1 7 1 1 1 2 1 2 1 2 3 3 2 7\n",
      " 6 2 1 1 5 1 2 1 1 1 1 6 6 2 2 3 1 1 2 1 3 1 1 4 3 3 2 3 1 1 1 3 4 1 6 4 1\n",
      " 3]\n"
     ]
    }
   ],
   "source": [
    "print(DBSCAN.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eps': 0.5, 'n_jobs': 1, 'algorithm': 'auto', 'min_samples': 5, 'leaf_size': 4, 'p': None, 'metric': 'euclidean'}\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "85.85858585858585\n",
      "3.367003367003367\n",
      "2.6936026936026938\n",
      "2.356902356902357\n",
      "2.0202020202020203\n",
      "1.6835016835016836\n",
      "2.0202020202020203\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Normalized mutual information score is 1.0 for DBSCAN cluster.\n",
      "V-measure score is 1.0 for DBSCAN cluster.\n",
      "Average Silhouette Coefficient for cluster 0 is -0.162.\n",
      "Average Silhouette Coefficient for cluster 1 is 0.6942.\n",
      "Average Silhouette Coefficient for cluster 2 is 0.6480.\n",
      "Average Silhouette Coefficient for cluster 3 is 0.5557.\n",
      "Average Silhouette Coefficient for cluster 4 is 0.6193.\n",
      "Average Silhouette Coefficient for cluster 5 is 0.3503.\n",
      "Average Silhouette Coefficient for cluster 6 is 0.7291.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "DBSCAN = cluster.DBSCAN(metric='euclidean', algorithm='auto', leaf_size=4, p=None).fit(df)\n",
    "# print(DBSCAN.labels_) \n",
    "pca = PCA(n_components=1)\n",
    "df_3_pca = np.ravel(pca.fit_transform(df))\n",
    "DDD = [df_3_pca.T,DBSCAN.labels_]\n",
    "df_3_db = pd.DataFrame(DDD)\n",
    "df_3_db = np.array(df_3_db.T.sort_values(by=[1], ascending=[True]).drop([1],1))\n",
    "df_3_distance = pairwise_distances(df_3_db, metric='euclidean')\n",
    "d_3_max = np.amax(df_3_distance)\n",
    "d_3_min = np.amin(df_3_distance)\n",
    "df_3_similarity = np.zeros([297,297])\n",
    "for i in range(len(df_3_distance)):\n",
    "    for j in range(len(df_3_distance)):\n",
    "        df_3_similarity[i,j] = 1-(df_3_distance[i,j]-d_3_min)/(d_3_max-d_3_min)\n",
    "\n",
    "# Similarity matrix\n",
    "sns.set(context=\"paper\", font=\"monospace\")\n",
    "f, ax = plt.subplots(figsize=(8,7))\n",
    "cmap = sns.diverging_palette(220, 8, as_cmap=True)\n",
    "sns.heatmap(df_3_similarity, cmap='GnBu',vmax=1, square=True)\n",
    "\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "plt.title('Similarity matrix for DBSCAN cluster (Q3)',fontsize=15)\n",
    "plt.savefig('d3.jpg')\n",
    "\n",
    "print(DBSCAN.get_params())\n",
    "cluster_percentage = [0]*15\n",
    "for i in range(15):\n",
    "    print(collections.Counter(DBSCAN.labels_)[i-5]*100/297)\n",
    "    \n",
    "\n",
    "DBSCAN_p = cluster.DBSCAN(metric='euclidean', algorithm='auto', leaf_size=4, p=None).fit_predict(df)\n",
    "# print(mutual_info_score(DBSCAN.labels_,DBSCAN_p))\n",
    "print(\"Normalized mutual information score is %.8s for DBSCAN cluster.\" % (normalized_mutual_info_score(DBSCAN.labels_,DBSCAN_p)))\n",
    "print(\"V-measure score is %.8s for DBSCAN cluster.\" % (v_measure_score(DBSCAN.labels_,DBSCAN_p)))\n",
    "SS_value = [silhouette_samples(df,DBSCAN.labels_),DBSCAN.labels_]\n",
    "SS_value = pd.DataFrame(SS_value)\n",
    "SS_value = np.array(SS_value.T.sort_values(by=[1], ascending=[True]).drop([1],1))\n",
    "cluster_num = [0]*len(collections.Counter(DBSCAN.labels_))\n",
    "for i in range(len(cluster_num)):\n",
    "    cluster_num[i] = collections.Counter(DBSCAN.labels_)[i-1]\n",
    "cluster_cum = np.cumsum(cluster_num)\n",
    "\n",
    "ran = len(cluster_cum)\n",
    "for i in range(ran):\n",
    "    if i == 0:\n",
    "        print(\"Average Silhouette Coefficient for cluster %.6s is %.6s.\" % (i,np.mean(SS_value[0:cluster_cum[i]])))\n",
    "    else:\n",
    "        print(\"Average Silhouette Coefficient for cluster %.6s is %.6s.\" % (i,np.mean(SS_value[cluster_cum[i-1]:cluster_cum[i]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized mutual information score is 1.0 for DBSCAN cluster.\n",
      "V-measure score is 1.0 for DBSCAN cluster.\n",
      "Average Silhouette Coefficient for cluster 0 is -0.162.\n",
      "Average Silhouette Coefficient for cluster 1 is 0.6942.\n",
      "Average Silhouette Coefficient for cluster 2 is 0.6480.\n",
      "Average Silhouette Coefficient for cluster 3 is 0.5557.\n",
      "Average Silhouette Coefficient for cluster 4 is 0.6193.\n",
      "Average Silhouette Coefficient for cluster 5 is 0.3503.\n",
      "Average Silhouette Coefficient for cluster 6 is 0.7291.\n"
     ]
    }
   ],
   "source": [
    "DBSCAN_p = cluster.DBSCAN(metric='euclidean', algorithm='auto', leaf_size=4, p=None).fit_predict(df)\n",
    "# print(mutual_info_score(DBSCAN.labels_,DBSCAN_p))\n",
    "print(\"Normalized mutual information score is %.8s for DBSCAN cluster.\" % (normalized_mutual_info_score(DBSCAN.labels_,DBSCAN_p)))\n",
    "print(\"V-measure score is %.8s for DBSCAN cluster.\" % (v_measure_score(DBSCAN.labels_,DBSCAN_p)))\n",
    "SS_value = [silhouette_samples(df,DBSCAN.labels_),DBSCAN.labels_]\n",
    "SS_value = pd.DataFrame(SS_value)\n",
    "SS_value = np.array(SS_value.T.sort_values(by=[1], ascending=[True]).drop([1],1))\n",
    "cluster_num = [0]*len(collections.Counter(DBSCAN.labels_))\n",
    "for i in range(len(cluster_num)):\n",
    "    cluster_num[i] = collections.Counter(DBSCAN.labels_)[i-1]\n",
    "cluster_cum = np.cumsum(cluster_num)\n",
    "\n",
    "ran = len(cluster_cum)\n",
    "for i in range(ran):\n",
    "    if i == 0:\n",
    "        print(\"Average Silhouette Coefficient for cluster %.6s is %.6s.\" % (i,np.mean(SS_value[0:cluster_cum[i]])))\n",
    "    else:\n",
    "        print(\"Average Silhouette Coefficient for cluster %.6s is %.6s.\" % (i,np.mean(SS_value[cluster_cum[i-1]:cluster_cum[i]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_time_d_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bc2f51d9a73d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"It took %.8s seconds to form DBSCAN cluster for Q3\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mload_time_d_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_time_d_3' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"It took %.8s seconds to form DBSCAN cluster for Q3\" % (load_time_d_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
